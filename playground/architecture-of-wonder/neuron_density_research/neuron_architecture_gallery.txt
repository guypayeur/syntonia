====================================================================================================
NEURAL ARCHITECTURE GALLERY: ASCII ART PORTRAITS
====================================================================================================

Each model is represented by its architectural signature:
  • WIDTH = Residual stream size (d_model)
  • HEIGHT = Depth (number of layers)
  • DENSITY = Expert count (█ = high, ▓ = medium, ▒ = low)

====================================================================================================


────────────────────────────────────────────────────────────────────────────────────────────────────
#1: ERNIE 4.5-21B-A3B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.225M  |  d_model: 2048  |  Layers: 48  |  Experts: 128

    ╔════════════════════╗
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ╚════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#2: Ling-mini-beta A0.8B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.169M  |  d_model: 2048  |  Layers: 20  |  Experts: 384

    ╔════════════════════╗
    ║████████████████████║
    ║████████████████████║
    ║████████████████████║
    ║████████████████████║
    ║████████████████████║
    ╚════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#3: LFM2-8B-A1B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.166M  |  d_model: 2048  |  Layers: 24  |  Experts: 32

    ╔════════════════════╗
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ╚════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#4: Qwen3-30B-A3B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.157M  |  d_model: 2048  |  Layers: 48  |  Experts: 128

    ╔════════════════════╗
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ╚════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#5: Tongyi DeepResearch 30B A3B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.157M  |  d_model: 2048  |  Layers: 48  |  Experts: 128

    ╔════════════════════╗
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ╚════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#6: Qwen3-Next-80B-A3B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.157M  |  d_model: 2048  |  Layers: 48  |  Experts: 512

    ╔════════════════════╗
    ║████████████████████║
    ║████████████████████║
    ║████████████████████║
    ║████████████████████║
    ║████████████████████║
    ║████████████████████║
    ╚════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#7: ERNIE 4.5-300B-A47B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.140M  |  d_model: 4096  |  Layers: 80  |  Experts: 256

    ╔═══════════════════════════╗
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ╚═══════════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#8: Ring-flash-linear-2.0
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.140M  |  d_model: 4096  |  Layers: 32  |  Experts: 256

    ╔═══════════════════════════╗
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║
    ╚═══════════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#9: GPT-OSS-120B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.113M  |  d_model: 2880  |  Layers: 36  |  Experts: 128

    ╔════════════════════╗
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ╚════════════════════╝


────────────────────────────────────────────────────────────────────────────────────────────────────
#10: GPT-OSS-20B
────────────────────────────────────────────────────────────────────────────────────────────────────
Neurons/B: 0.105M  |  d_model: 2880  |  Layers: 24  |  Experts: 32

    ╔════════════════════╗
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ║▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒║
    ╚════════════════════╝
